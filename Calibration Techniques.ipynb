{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Calibration Methods\n",
    "\n",
    "### Histogram Binning ([paper](http://cseweb.ucsd.edu/~elkan/calibrated.pdf))\n",
    "\n",
    "Input: softmaxed logits\n",
    "\n",
    "1. Put dev scores in bins.\n",
    "2. Calculate average dev label $\\hat{p}_i \\in [0,1]$ for bin $i$.\n",
    "3. Put test scores in bins with same boundaries as dev scores.\n",
    "4. Assign $\\hat{p}_i$ as calibrated score for each uncalibrated test score in bin $i$.\n",
    "\n",
    "### Isotonic Regression ([paper](https://www.jstor.org/stable/2284712?seq=1#metadata_info_tab_contents))\n",
    "\n",
    "Input: softmaxed logits\n",
    "\n",
    "Isotonic = order preserving / monotonically increasing\n",
    "\n",
    "Learn a monotonically increasing piecewise function that pulls uncalibrated scores toward $y = x$ on a reliability diagram.\n",
    "\n",
    "![Isotonic Regression](figures/isotonic_regression.png)\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/isotonic.html for more.\n",
    "\n",
    "### Temperature Scaling ([paper](https://arxiv.org/pdf/1706.04599.pdf))\n",
    "\n",
    "Input: pre-softmaxed logits\n",
    "\n",
    "Using the validation set, learn (by minimizing negative log likelihood) a single scalar parameter T to multiply logits by.\n",
    "\n",
    "Multiply logits by the learned T parameter and then softmax to get calibrated confidence scores.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "calibration",
   "language": "python",
   "name": "calibration"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
